{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Million Song Dataset\n",
    "\n",
    "Dataset comes from: http://millionsongdataset.com/. Currently, looking at just the subset. It contains the musical features (not the actual music files). Additionally, it contains an artist musicbrain id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# million song dataset manipulation\n",
    "import h5py\n",
    "import numpy as np\n",
    "import h5json\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing\n",
    "import psycopg2\n",
    "\n",
    "# cover art\n",
    "import musicbrainzngs\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "from Levenshtein import distance, ratio\n",
    "\n",
    "from datetime import datetime\n",
    "import urllib\n",
    "\n",
    "# set a real one at some point\n",
    "musicbrainzngs.set_useragent(\"thesis\", \"0.1\", \"gnaswerdna@gmail.com\")\n",
    "\n",
    "path = '/Users/andrewsang/datasci/thesis'\n",
    "\n",
    "pd.options.display.max_rows = 999\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cover Art and Genre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cover Art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "artist_mbid = '286ec4c2-b5ca-4f85-b331-280a6d73dd14'\n",
    "msd_release_str = 'Joao Voz E Violato'\n",
    "track_id = 'TRACCJA128F149A144'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cover_art_img_url(release_id):\n",
    "    try:\n",
    "        return musicbrainzngs.get_image_list(release_id)['images'][0]['image']\n",
    "    except:\n",
    "        return np.NaN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2: Use Data Dump"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, download dump and recreate using these directions: https://github.com/lalinsky/mbdata. Might need this for postgres: https://stackoverflow.com/a/46971838"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected!\n"
     ]
    }
   ],
   "source": [
    "conn = psycopg2.connect(dbname=\"musicbrainz\", user=\"andrewsang\", host=\"127.0.0.1\", port=5432)\n",
    "print(\"Connected!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"SELECT DISTINCT id as artist_id, gid as artist_mbid, name as artist_name, comment \n",
    "           FROM musicbrainz.artist\"\"\"\n",
    "artist = pd.read_sql(query, conn)\n",
    "\n",
    "query = \"\"\"SELECT id as release_id, gid as release_mbid, name as release_name, artist_credit as artist_id  \n",
    "           FROM musicbrainz.release\n",
    "           GROUP BY 1,2,3,4\"\"\"\n",
    "release = pd.read_sql(query, conn)\n",
    "\n",
    "query = \"\"\"SELECT id as cover_art_id, release as release_id\n",
    "           FROM cover_art_archive.cover_art\n",
    "           GROUP BY 1,2\"\"\"\n",
    "cover_art = pd.read_sql(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "release_w_art = (release.merge(cover_art, how='inner', on=['release_id'])\n",
    "                        .loc[:,['release_id','release_name','artist_id','release_mbid']]\n",
    "                        .drop_duplicates()\n",
    "                        .merge(artist, how='inner', on=['artist_id'])\n",
    "                        .loc[:,['artist_mbid','release_id','release_name','release_mbid']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ltma: lyrics_track_metadata_w_art\n",
    "ltma = lyrics_track_metadata.merge(release_w_art, on=['artist_mbid'])\n",
    "\n",
    "ltma.loc[:,'lev_dist'] = [distance(k,v) for k,v in zip(ltma.release_name, ltma.release)]\n",
    "ltma.loc[:,'str_ratio'] = [ratio(k,v) for k,v in zip(ltma.release_name, ltma.release)]\n",
    "ltma.loc[pd.isnull(ltma.loc[:,'str_ratio']),'str_ratio'] = np.NaN\n",
    "\n",
    "ltma.loc[:,'search_rnk'] = ltma.groupby(['track_id'])['str_ratio'].rank(ascending=False, method='first')\n",
    "\n",
    "ltma = ltma.loc[(ltma['str_ratio'] > .5) & (ltma['search_rnk'] == 1), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker(num):\n",
    "    print('Started', num, 'at ',datetime.now().strftime('%H:%M:%S'))\n",
    "    df = split_dataset_into_fourths(num, art_url_df)\n",
    "    df.loc[:,'cover_url'] = df.loc[:,'release_mbid'].map(lambda s: cover_art_img_url(s))\n",
    "    now = datetime.now().strftime('%Y%M%d_%H%M%S')\n",
    "    df.to_csv('{0}/output_{1}_{2}.csv'.format(folder, now, num))\n",
    "    print('Finished with', num, 'at ',datetime.now().strftime('%H:%M:%S'))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run multiprocessing\n",
    "# n = multiprocessing.cpu_count()\n",
    "# result = pd.DataFrame()\n",
    "# folder = 'rel_mb'\n",
    "# jobs = []\n",
    "# for i in range(n):\n",
    "#     p = multiprocessing.Process(target=worker, args=(i,))\n",
    "#     jobs.append(p)\n",
    "#     p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'rel_mb'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-5c9ac4bde49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0murl_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rel_mb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'.csv'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0murl_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0murl_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rel_mb/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'rel_mb'"
     ]
    }
   ],
   "source": [
    "url_df = pd.DataFrame()\n",
    "for f in os.listdir('rel_mb'):\n",
    "    if f[-4:] == '.csv':\n",
    "        url_df = pd.concat([url_df, pd.read_csv('rel_mb/'+f)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r, url in zip(url_df.release_mbid, url_df.cover_url):\n",
    "    try:\n",
    "        urllib.request.urlretrieve(url, \"../data/covers/{0}.jpg\".format(r))\n",
    "#         urllib.request.urlretrieve(url, \"/volumes/TOSHIBA EXT/data/covers/{0}.jpg\".format(r))\n",
    "    except:\n",
    "        pass"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thesis",
   "language": "python",
   "name": "thesis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
