{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt \n",
    "* HW assignment: Using dataset selected for final project:\n",
    "\n",
    "\n",
    "* Perform feature engineering\n",
    "* Estimate baseline model\n",
    "* Estimate different model and/or different loss function to improve model performance\n",
    "* Hint: Determine what metric(s) is/are appropriate for your use case\n",
    "* Interpret results\n",
    "* Explain what you did and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write-up\n",
    "* Goal: Forecast the last 6 weeks of the time series for each store.\n",
    "* Kaggle Prompt: Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what’s most important to them: their customers and their teams! \n",
    "\n",
    "**Variable Importance**\n",
    "\n",
    "I first utilize a GBM in order to see if it can quickly tell me which variables are important or not. I do not set this up as a normal time series forecasting question, so I don't add any lags (prev day, week, month). I also don't split the dataset by time series and I don't force the train/test to not overlap. However, using the output, I am able to see that the following variables are important: \n",
    "\n",
    "    1. \"Open\": if store is open\n",
    "    2. \"Promo\": if that Day is a promo day\n",
    "    3. \"CompetitionDistance\": nearest competitor distance\n",
    "    \n",
    "**Feature Engineering**\n",
    "\n",
    "I honestly couldn't think of a ton of different features to add. I did think that adding in that day's distance to the previous open day and the previous promo day could be promising, given how important those features seemed to be.\n",
    "\n",
    "**1st Attempt: Prophet w/o addtl regressors**\n",
    "As a baseline model, I decided to use <a href=\"https://research.fb.com/prophet-forecasting-at-scale/\">Prophet from fb</a>. It's a time series model that I use as a baseline model because it doesn't make a ton of assumptions about the underlying data structure.\n",
    "\n",
    "**2nd Attempt: Prophet w/ addtl regressors**\n",
    "Additionally, it is very simple to implement and has the ability to add in additional regressors. The second version of the model included the regressors for the top 3 important variables as well as some of the features that I created, \"daydiff_open_dt\" and \"daydiff_promo_dt\". These variables represent the distance from the date being forecast to the previous day the store was open as well as the previous day that the store had a promo.\n",
    "\n",
    "**Metric Selection**\n",
    "The competition utilizes RMSPE (Root Mean Squared Percentage Error) and doesn't include any day with 0 sales. I think that this is good in that it penalizes large errors. However, I do not like the fact that we are looking at the error relative to the prediction, so I also recommend looking at RMSE. Since the competition says the purpose is to help the managers effectively staff their stores, I make the assumption that there are a certain number of salespeople per sales volume. The problem with RMSPE is: if there were only 1 store/day and we forecast 50,000 in sales and the actual came in at 100,000, the RMSPE would be 50%. Now, if there were only 1 store/day and we forecast 50 in sales and the actual came in at 100, the RMSPE would **also** be 50%. To me, the first seems like a much bigger error in terms of staffing (if there is some $/salespeople ratio). RMSE for the first scenario is 50,000 and 50 for the second.\n",
    "\n",
    "\n",
    "\n",
    "* I noticed that there seemed to be an "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/paperspace/ANDREW-MINKYU-SANG/final_proj/workprod'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/paperspace/ANDREW-MINKYU-SANG\n"
     ]
    }
   ],
   "source": [
    "cd /home/paperspace/ANDREW-MINKYU-SANG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /Users/andrewsang/Documents/ucla_stats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mClass0-Intro\u001b[0m/  \u001b[01;34mClass2\u001b[0m/  \u001b[01;34mClass4\u001b[0m/  \u001b[01;34mfinal_proj\u001b[0m/  req.txt\r\n",
      "\u001b[01;34mClass1\u001b[0m/        \u001b[01;34mClass3\u001b[0m/  \u001b[01;34mClass5\u001b[0m/  README.md    requirements.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('final_proj/input_data/train.csv')\n",
    "test = pd.read_csv('final_proj/input_data/test.csv')\n",
    "store = pd.read_csv('final_proj/input_data/store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Store</th>\n",
       "      <th>DayOfWeek</th>\n",
       "      <th>Date</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Customers</th>\n",
       "      <th>Open</th>\n",
       "      <th>Promo</th>\n",
       "      <th>StateHoliday</th>\n",
       "      <th>SchoolHoliday</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>5263</td>\n",
       "      <td>555</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>6064</td>\n",
       "      <td>625</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>8314</td>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>13995</td>\n",
       "      <td>1498</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2015-07-31</td>\n",
       "      <td>4822</td>\n",
       "      <td>559</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Store  DayOfWeek        Date  Sales  Customers  Open  Promo StateHoliday  \\\n",
       "0      1          5  2015-07-31   5263        555     1      1            0   \n",
       "1      2          5  2015-07-31   6064        625     1      1            0   \n",
       "2      3          5  2015-07-31   8314        821     1      1            0   \n",
       "3      4          5  2015-07-31  13995       1498     1      1            0   \n",
       "4      5          5  2015-07-31   4822        559     1      1            0   \n",
       "\n",
       "   SchoolHoliday  \n",
       "0              1  \n",
       "1              1  \n",
       "2              1  \n",
       "3              1  \n",
       "4              1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train.loc[:,'Date'] = pd.to_datetime(train['Date'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Manipulation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manipulation(dataframe):\n",
    "    dataframe = dataframe.merge(store,how='left',on='Store')\n",
    "    mask = pd.isnull(dataframe['CompetitionDistance'])\n",
    "    # replace 3 stores with median where competition distance is null\n",
    "    dataframe.loc[mask,'CompetitionDistance'] = store['CompetitionDistance'].median() \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_dist(dataframe, start_dt):\n",
    "    # measuring current date - \n",
    "    dataframe = dataframe.sort_values(['Store','Date'])\n",
    "    dataframe.loc[:,'prev_'+start_dt] = dataframe.groupby('Store')[start_dt] \\\n",
    "                                                 .transform(lambda x:x.ffill().shift(1))\n",
    "    dataframe.loc[:,'daydiff_'+start_dt] = (dataframe['Date']- dataframe['prev_'+start_dt]) / \\\n",
    "                                            np.timedelta64(1, 'D') - 1\n",
    "    dataframe.loc[pd.isnull(dataframe['daydiff_'+start_dt]),'daydiff_'+start_dt] = np.NaN\n",
    "    return dataframe\n",
    "\n",
    "def ftr_eng(dataframe):\n",
    "    dataframe.loc[:,'promo_dt'] = np.where(dataframe['Promo']==1,dataframe['Date'], np.NaN)\n",
    "    dataframe.loc[:,'closed_dt'] = np.where(dataframe['Open']==0,dataframe['Date'], np.NaN)\n",
    "    dataframe.loc[:,'open_dt'] = np.where(dataframe['Open']==1,dataframe['Date'], np.NaN)\n",
    "    dataframe.head()\n",
    "    for k in ['Date','closed_dt','open_dt','promo_dt']:\n",
    "        dataframe.loc[:,k] = pd.to_datetime(dataframe.loc[:,k])\n",
    "        \n",
    "    dataframe = day_dist(dataframe,'open_dt') # distance to most recent open day\n",
    "    dataframe = day_dist(dataframe,'promo_dt') # distance to most recent promo day\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv_df(dataframe):\n",
    "    return pd.pivot_table(dataframe,values='Sales',index=['Store'], columns=['Date']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GBM for Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_proj/input_data/train.csv')\n",
    "df = data_manipulation(df)\n",
    "df = ftr_eng(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking whether there is an H2O instance running at http://localhost:54321..... not found.\n",
      "Attempting to start a local H2O server...\n",
      "  Java Version: openjdk version \"1.8.0_191\"; OpenJDK Runtime Environment (build 1.8.0_191-8u191-b12-2ubuntu0.16.04.1-b12); OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode)\n",
      "  Starting server from /home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/h2o/backend/bin/h2o.jar\n",
      "  Ice root: /tmp/tmp3xk96i_3\n",
      "  JVM stdout: /tmp/tmp3xk96i_3/h2o_paperspace_started_from_python.out\n",
      "  JVM stderr: /tmp/tmp3xk96i_3/h2o_paperspace_started_from_python.err\n",
      "  Server is running at http://127.0.0.1:54321\n",
      "Connecting to H2O server at http://127.0.0.1:54321... successful.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td>H2O cluster uptime:</td>\n",
       "<td>01 secs</td></tr>\n",
       "<tr><td>H2O cluster timezone:</td>\n",
       "<td>America/New_York</td></tr>\n",
       "<tr><td>H2O data parsing timezone:</td>\n",
       "<td>UTC</td></tr>\n",
       "<tr><td>H2O cluster version:</td>\n",
       "<td>3.22.1.2</td></tr>\n",
       "<tr><td>H2O cluster version age:</td>\n",
       "<td>25 days </td></tr>\n",
       "<tr><td>H2O cluster name:</td>\n",
       "<td>H2O_from_python_paperspace_4wg7xo</td></tr>\n",
       "<tr><td>H2O cluster total nodes:</td>\n",
       "<td>1</td></tr>\n",
       "<tr><td>H2O cluster free memory:</td>\n",
       "<td>6.543 Gb</td></tr>\n",
       "<tr><td>H2O cluster total cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster allowed cores:</td>\n",
       "<td>8</td></tr>\n",
       "<tr><td>H2O cluster status:</td>\n",
       "<td>accepting new members, healthy</td></tr>\n",
       "<tr><td>H2O connection url:</td>\n",
       "<td>http://127.0.0.1:54321</td></tr>\n",
       "<tr><td>H2O connection proxy:</td>\n",
       "<td>None</td></tr>\n",
       "<tr><td>H2O internal security:</td>\n",
       "<td>False</td></tr>\n",
       "<tr><td>H2O API Extensions:</td>\n",
       "<td>XGBoost, Algos, AutoML, Core V3, Core V4</td></tr>\n",
       "<tr><td>Python version:</td>\n",
       "<td>3.6.8 final</td></tr></table></div>"
      ],
      "text/plain": [
       "--------------------------  ----------------------------------------\n",
       "H2O cluster uptime:         01 secs\n",
       "H2O cluster timezone:       America/New_York\n",
       "H2O data parsing timezone:  UTC\n",
       "H2O cluster version:        3.22.1.2\n",
       "H2O cluster version age:    25 days\n",
       "H2O cluster name:           H2O_from_python_paperspace_4wg7xo\n",
       "H2O cluster total nodes:    1\n",
       "H2O cluster free memory:    6.543 Gb\n",
       "H2O cluster total cores:    8\n",
       "H2O cluster allowed cores:  8\n",
       "H2O cluster status:         accepting new members, healthy\n",
       "H2O connection url:         http://127.0.0.1:54321\n",
       "H2O connection proxy:\n",
       "H2O internal security:      False\n",
       "H2O API Extensions:         XGBoost, Algos, AutoML, Core V3, Core V4\n",
       "Python version:             3.6.8 final\n",
       "--------------------------  ----------------------------------------"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parse progress: |█████████████████████████████████████████████████████████| 100%\n",
      "gbm Model Build progress: |███████████████████████████████████████████████| 100%\n",
      "Model Details\n",
      "=============\n",
      "H2OGradientBoostingEstimator :  Gradient Boosting Machine\n",
      "Model Key:  GBM_model_python_1550116348767_1\n",
      "\n",
      "\n",
      "ModelMetricsRegression: gbm\n",
      "** Reported on train data. **\n",
      "\n",
      "MSE: 4727991.869292869\n",
      "RMSE: 2174.3945983406206\n",
      "MAE: 1461.935261250391\n",
      "RMSLE: NaN\n",
      "Mean Residual Deviance: 4727991.869292869\n",
      "\n",
      "ModelMetricsRegression: gbm\n",
      "** Reported on validation data. **\n",
      "\n",
      "MSE: 4713480.94515923\n",
      "RMSE: 2171.055260733644\n",
      "MAE: 1458.884191873281\n",
      "RMSLE: NaN\n",
      "Mean Residual Deviance: 4713480.94515923\n",
      "Scoring History: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b></b></td>\n",
       "<td><b>timestamp</b></td>\n",
       "<td><b>duration</b></td>\n",
       "<td><b>number_of_trees</b></td>\n",
       "<td><b>training_rmse</b></td>\n",
       "<td><b>training_mae</b></td>\n",
       "<td><b>training_deviance</b></td>\n",
       "<td><b>validation_rmse</b></td>\n",
       "<td><b>validation_mae</b></td>\n",
       "<td><b>validation_deviance</b></td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:09</td>\n",
       "<td> 0.018 sec</td>\n",
       "<td>0.0</td>\n",
       "<td>3848.8251131</td>\n",
       "<td>2885.9870348</td>\n",
       "<td>14813454.7514988</td>\n",
       "<td>3851.0255084</td>\n",
       "<td>2889.8053439</td>\n",
       "<td>14830397.4666661</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:11</td>\n",
       "<td> 2.210 sec</td>\n",
       "<td>1.0</td>\n",
       "<td>3629.3644376</td>\n",
       "<td>2724.6232774</td>\n",
       "<td>13172286.2210690</td>\n",
       "<td>3630.8297633</td>\n",
       "<td>2727.6975636</td>\n",
       "<td>13182924.7700954</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:12</td>\n",
       "<td> 2.704 sec</td>\n",
       "<td>2.0</td>\n",
       "<td>3441.1792135</td>\n",
       "<td>2585.5925094</td>\n",
       "<td>11841714.3796366</td>\n",
       "<td>3441.8948054</td>\n",
       "<td>2587.9473207</td>\n",
       "<td>11846639.8515635</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:12</td>\n",
       "<td> 2.961 sec</td>\n",
       "<td>3.0</td>\n",
       "<td>3280.2457228</td>\n",
       "<td>2464.2569246</td>\n",
       "<td>10760012.0016976</td>\n",
       "<td>3280.4042185</td>\n",
       "<td>2466.0787872</td>\n",
       "<td>10761051.8369250</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:12</td>\n",
       "<td> 3.181 sec</td>\n",
       "<td>4.0</td>\n",
       "<td>3143.2508450</td>\n",
       "<td>2360.7670172</td>\n",
       "<td>9880025.8745346</td>\n",
       "<td>3142.6874460</td>\n",
       "<td>2361.9834542</td>\n",
       "<td>9876484.3831225</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:13</td>\n",
       "<td> 3.379 sec</td>\n",
       "<td>5.0</td>\n",
       "<td>3026.4024134</td>\n",
       "<td>2270.1669576</td>\n",
       "<td>9159111.5680627</td>\n",
       "<td>3025.3213713</td>\n",
       "<td>2270.9906560</td>\n",
       "<td>9152569.3998255</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:13</td>\n",
       "<td> 3.580 sec</td>\n",
       "<td>6.0</td>\n",
       "<td>2927.6902372</td>\n",
       "<td>2191.6179454</td>\n",
       "<td>8571370.1251473</td>\n",
       "<td>2926.3661259</td>\n",
       "<td>2191.8715543</td>\n",
       "<td>8563618.7030274</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:13</td>\n",
       "<td> 3.754 sec</td>\n",
       "<td>7.0</td>\n",
       "<td>2844.1301747</td>\n",
       "<td>2123.0809102</td>\n",
       "<td>8089076.4507835</td>\n",
       "<td>2842.2654479</td>\n",
       "<td>2122.9725609</td>\n",
       "<td>8078472.8761332</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:13</td>\n",
       "<td> 3.920 sec</td>\n",
       "<td>8.0</td>\n",
       "<td>2772.5379590</td>\n",
       "<td>2061.5546515</td>\n",
       "<td>7686966.7339068</td>\n",
       "<td>2770.3757016</td>\n",
       "<td>2061.0668882</td>\n",
       "<td>7674981.5278258</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:13</td>\n",
       "<td> 4.115 sec</td>\n",
       "<td>9.0</td>\n",
       "<td>2713.4362212</td>\n",
       "<td>2008.2278540</td>\n",
       "<td>7362736.1267706</td>\n",
       "<td>2710.9676229</td>\n",
       "<td>2007.4856651</td>\n",
       "<td>7349345.4526169</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:17</td>\n",
       "<td> 8.217 sec</td>\n",
       "<td>40.0</td>\n",
       "<td>2247.7544142</td>\n",
       "<td>1510.0213523</td>\n",
       "<td>5052399.9064799</td>\n",
       "<td>2244.2218588</td>\n",
       "<td>1507.4041271</td>\n",
       "<td>5036531.7513271</td></tr>\n",
       "<tr><td></td>\n",
       "<td>2019-02-13 22:53:20</td>\n",
       "<td>10.358 sec</td>\n",
       "<td>50.0</td>\n",
       "<td>2174.3945983</td>\n",
       "<td>1461.9352613</td>\n",
       "<td>4727991.8692929</td>\n",
       "<td>2171.0552607</td>\n",
       "<td>1458.8841919</td>\n",
       "<td>4713480.9451592</td></tr></table></div>"
      ],
      "text/plain": [
       "    timestamp            duration    number_of_trees    training_rmse    training_mae    training_deviance    validation_rmse    validation_mae    validation_deviance\n",
       "--  -------------------  ----------  -----------------  ---------------  --------------  -------------------  -----------------  ----------------  ---------------------\n",
       "    2019-02-13 22:53:09  0.018 sec   0                  3848.83          2885.99         1.48135e+07          3851.03            2889.81           1.48304e+07\n",
       "    2019-02-13 22:53:11  2.210 sec   1                  3629.36          2724.62         1.31723e+07          3630.83            2727.7            1.31829e+07\n",
       "    2019-02-13 22:53:12  2.704 sec   2                  3441.18          2585.59         1.18417e+07          3441.89            2587.95           1.18466e+07\n",
       "    2019-02-13 22:53:12  2.961 sec   3                  3280.25          2464.26         1.076e+07            3280.4             2466.08           1.07611e+07\n",
       "    2019-02-13 22:53:12  3.181 sec   4                  3143.25          2360.77         9.88003e+06          3142.69            2361.98           9.87648e+06\n",
       "    2019-02-13 22:53:13  3.379 sec   5                  3026.4           2270.17         9.15911e+06          3025.32            2270.99           9.15257e+06\n",
       "    2019-02-13 22:53:13  3.580 sec   6                  2927.69          2191.62         8.57137e+06          2926.37            2191.87           8.56362e+06\n",
       "    2019-02-13 22:53:13  3.754 sec   7                  2844.13          2123.08         8.08908e+06          2842.27            2122.97           8.07847e+06\n",
       "    2019-02-13 22:53:13  3.920 sec   8                  2772.54          2061.55         7.68697e+06          2770.38            2061.07           7.67498e+06\n",
       "    2019-02-13 22:53:13  4.115 sec   9                  2713.44          2008.23         7.36274e+06          2710.97            2007.49           7.34935e+06\n",
       "    2019-02-13 22:53:17  8.217 sec   40                 2247.75          1510.02         5.0524e+06           2244.22            1507.4            5.03653e+06\n",
       "    2019-02-13 22:53:20  10.358 sec  50                 2174.39          1461.94         4.72799e+06          2171.06            1458.88           4.71348e+06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable Importances: \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div style=\"overflow:auto\"><table style=\"width:50%\"><tr><td><b>variable</b></td>\n",
       "<td><b>relative_importance</b></td>\n",
       "<td><b>scaled_importance</b></td>\n",
       "<td><b>percentage</b></td></tr>\n",
       "<tr><td>Open</td>\n",
       "<td>21930090430464.0000000</td>\n",
       "<td>1.0</td>\n",
       "<td>0.6765801</td></tr>\n",
       "<tr><td>Promo</td>\n",
       "<td>3505222582272.0000000</td>\n",
       "<td>0.1598362</td>\n",
       "<td>0.1081420</td></tr>\n",
       "<tr><td>CompetitionDistance</td>\n",
       "<td>1262618411008.0000000</td>\n",
       "<td>0.0575747</td>\n",
       "<td>0.0389539</td></tr>\n",
       "<tr><td>DayOfWeek</td>\n",
       "<td>934970982400.0000000</td>\n",
       "<td>0.0426342</td>\n",
       "<td>0.0288454</td></tr>\n",
       "<tr><td>StoreType</td>\n",
       "<td>920981995520.0000000</td>\n",
       "<td>0.0419963</td>\n",
       "<td>0.0284138</td></tr>\n",
       "<tr><td>Assortment</td>\n",
       "<td>794724663296.0000000</td>\n",
       "<td>0.0362390</td>\n",
       "<td>0.0245186</td></tr>\n",
       "<tr><td>Promo2SinceWeek</td>\n",
       "<td>632819679232.0000000</td>\n",
       "<td>0.0288562</td>\n",
       "<td>0.0195236</td></tr>\n",
       "<tr><td>Promo2SinceYear</td>\n",
       "<td>592458612736.0000000</td>\n",
       "<td>0.0270158</td>\n",
       "<td>0.0182783</td></tr>\n",
       "<tr><td>CompetitionOpenSinceMonth</td>\n",
       "<td>459221336064.0000000</td>\n",
       "<td>0.0209402</td>\n",
       "<td>0.0141677</td></tr>\n",
       "<tr><td>prev_open_dt</td>\n",
       "<td>378988036096.0000000</td>\n",
       "<td>0.0172816</td>\n",
       "<td>0.0116924</td></tr>\n",
       "<tr><td>CompetitionOpenSinceYear</td>\n",
       "<td>283791065088.0000000</td>\n",
       "<td>0.0129407</td>\n",
       "<td>0.0087554</td></tr>\n",
       "<tr><td>Date</td>\n",
       "<td>192784269312.0000000</td>\n",
       "<td>0.0087909</td>\n",
       "<td>0.0059477</td></tr>\n",
       "<tr><td>daydiff_open_dt</td>\n",
       "<td>159628607488.0000000</td>\n",
       "<td>0.0072790</td>\n",
       "<td>0.0049248</td></tr>\n",
       "<tr><td>PromoInterval</td>\n",
       "<td>138798891008.0000000</td>\n",
       "<td>0.0063292</td>\n",
       "<td>0.0042822</td></tr>\n",
       "<tr><td>prev_promo_dt</td>\n",
       "<td>137994649600.0000000</td>\n",
       "<td>0.0062925</td>\n",
       "<td>0.0042574</td></tr>\n",
       "<tr><td>daydiff_promo_dt</td>\n",
       "<td>49835700224.0000000</td>\n",
       "<td>0.0022725</td>\n",
       "<td>0.0015375</td></tr>\n",
       "<tr><td>SchoolHoliday</td>\n",
       "<td>37443715072.0000000</td>\n",
       "<td>0.0017074</td>\n",
       "<td>0.0011552</td></tr>\n",
       "<tr><td>Promo2</td>\n",
       "<td>658971456.0000000</td>\n",
       "<td>0.0000300</td>\n",
       "<td>0.0000203</td></tr>\n",
       "<tr><td>StateHoliday</td>\n",
       "<td>113404328.0000000</td>\n",
       "<td>0.0000052</td>\n",
       "<td>0.0000035</td></tr></table></div>"
      ],
      "text/plain": [
       "variable                   relative_importance    scaled_importance    percentage\n",
       "-------------------------  ---------------------  -------------------  ------------\n",
       "Open                       2.19301e+13            1                    0.67658\n",
       "Promo                      3.50522e+12            0.159836             0.108142\n",
       "CompetitionDistance        1.26262e+12            0.0575747            0.0389539\n",
       "DayOfWeek                  9.34971e+11            0.0426342            0.0288454\n",
       "StoreType                  9.20982e+11            0.0419963            0.0284138\n",
       "Assortment                 7.94725e+11            0.036239             0.0245186\n",
       "Promo2SinceWeek            6.3282e+11             0.0288562            0.0195236\n",
       "Promo2SinceYear            5.92459e+11            0.0270158            0.0182783\n",
       "CompetitionOpenSinceMonth  4.59221e+11            0.0209402            0.0141677\n",
       "prev_open_dt               3.78988e+11            0.0172816            0.0116924\n",
       "CompetitionOpenSinceYear   2.83791e+11            0.0129407            0.00875543\n",
       "Date                       1.92784e+11            0.00879086           0.00594772\n",
       "daydiff_open_dt            1.59629e+11            0.00727898           0.00492481\n",
       "PromoInterval              1.38799e+11            0.00632915           0.00428218\n",
       "prev_promo_dt              1.37995e+11            0.00629248           0.00425737\n",
       "daydiff_promo_dt           4.98357e+10            0.00227248           0.00153752\n",
       "SchoolHoliday              3.74437e+10            0.00170741           0.0011552\n",
       "Promo2                     6.58971e+08            3.00487e-05          2.03304e-05\n",
       "StateHoliday               1.13404e+08            5.17117e-06          3.49871e-06"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import h2o\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "hf = h2o.H2OFrame(df)\n",
    "\n",
    "# Set up X and Y columns\n",
    "X = [e for e in df]\n",
    "X.remove('Store')\n",
    "X.remove('Sales')\n",
    "X.remove('Customers')\n",
    "X.remove('open_dt')\n",
    "X.remove('closed_dt')\n",
    "X.remove('promo_dt')\n",
    "y = 'Sales'\n",
    "\n",
    "# Split Frame\n",
    "train, valid, test = hf.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "# Specify Model\n",
    "gbm = H2OGradientBoostingEstimator(seed=123)\n",
    "gbm.train(X, y, training_frame=train, validation_frame=valid)\n",
    "\n",
    "# Summary\n",
    "gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophetFormat(dataframe):\n",
    "    dataframe = dataframe.rename(index=str, columns={\"Sales\": \"y\",\"Date\": \"ds\"})\n",
    "    dataframe.loc[:,'ds'] = pd.to_datetime(dataframe['ds'])\n",
    "    for c in ['Promo','Open','CompetitionDistance']:\n",
    "        dataframe.loc[:,c] = dataframe.loc[:,c].astype(float)\n",
    "    dataframe.loc[:,'floor'] = 0\n",
    "    dataframe = dataframe.fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3020: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "og = pd.read_csv('final_proj/input_data/train.csv')\n",
    "og = data_manipulation(og)\n",
    "og = ftr_eng(og)\n",
    "og = prophetFormat(og)\n",
    "\n",
    "date_change = pd.to_datetime('2015-06-20') # this date leaves 42 days post\n",
    "mask = og['ds']>=date_change\n",
    "test = og[mask]\n",
    "train = og[mask==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list = train['Store'].unique().tolist()\n",
    "\n",
    "def run_indiv_forecast(idx, addtl_reg=False):\n",
    "    # pull data\n",
    "    sample = train.loc[train['Store']==store_list[idx],:]\n",
    "    test_sample = test.loc[test['Store']==store_list[idx],:]\n",
    "    \n",
    "    # create model, add regressors\n",
    "    m = Prophet(daily_seasonality=False)\n",
    "    if addtl_reg:\n",
    "        m.add_regressor('Promo',mode='multiplicative')\n",
    "        m.add_regressor('Open',mode='multiplicative')\n",
    "        m.add_regressor('CompetitionDistance')\n",
    "        m.add_regressor('daydiff_open_dt')\n",
    "        m.add_regressor('daydiff_promo_dt')\n",
    "    m.fit(sample)\n",
    "    \n",
    "    # create results, merge w actuals\n",
    "    forecast = m.predict(test_sample)\n",
    "    forecast.loc[:,'Store'] = train.loc[train['Store']==store_list[idx],'Store'].max()\n",
    "    fc = forecast[['Store','ds','yhat']]\n",
    "    fc = fc.merge(test_sample.loc[:,['ds','y']],how='left',on=['ds'])\n",
    "    return fc\n",
    "\n",
    "def run_individ_forecast_w_reg(idx):\n",
    "    return run_indiv_forecast(idx,addtl_reg=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Attempt: Prophet w/o addtl regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization comes in spirit from: https://medium.com/devschile/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- 287.1863389015198 seconds --\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Run Version without External Regressors\n",
    "store_list = train.Store.unique()\n",
    "start_time = time.time()\n",
    "\n",
    "p = Pool(cpu_count())\n",
    "seriesidx = np.arange(0,train['Store'].nunique())\n",
    "predictions = list(p.imap(run_indiv_forecast, seriesidx))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "print(\"-- %s seconds --\" % (time.time() - start_time))\n",
    "\n",
    "results = pd.DataFrame()\n",
    "\n",
    "for e in np.arange(0,len(predictions)):\n",
    "    results = pd.concat([results,predictions[e]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to csv\n",
    "results.to_csv('results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation Metrics\n",
    "def rmse(dataframe):\n",
    "    interim = dataframe.loc[dataframe['y']!=0,:]\n",
    "    return np.sqrt(np.mean((interim['y'] - interim['yhat'])**2))\n",
    "\n",
    "def rmspe(dataframe):\n",
    "    interim = dataframe.loc[dataframe['y']!=0,:]\n",
    "    return np.sqrt(np.mean(((interim['y']-interim['yhat'])/interim['y'])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse is 1630.4690648652977\n",
      "rmspe is 0.2315375072219016\n"
     ]
    }
   ],
   "source": [
    "print('rmse is '+str(rmse(results)))\n",
    "print('rmspe is '+str(rmspe(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Attempt: Prophet w/ addtl regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/fbprophet/forecaster.py:880: FutureWarning: Series.nonzero() is deprecated and will be removed in a future version.Use Series.to_numpy().nonzero() instead\n",
      "  min_dt = dt.iloc[dt.nonzero()[0]].min()\n",
      "/home/paperspace/anaconda3/envs/fastai/lib/python3.6/site-packages/pystan/misc.py:399: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  elif np.issubdtype(np.asarray(v).dtype, float):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 765.4674119949341 seconds ---\n"
     ]
    }
   ],
   "source": [
    "# Run Version with External Regressors\n",
    "store_list = train.Store.unique()\n",
    "results2 = pd.DataFrame()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "p = Pool(cpu_count())\n",
    "seriesidx = np.arange(0,train['Store'].nunique())\n",
    "predictions = list(p.imap(run_individ_forecast_w_reg, seriesidx))\n",
    "p.close()\n",
    "p.join()\n",
    "\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "results2 = pd.DataFrame()\n",
    "\n",
    "for e in np.arange(0,len(predictions)):\n",
    "    results2 = pd.concat([results2,predictions[e]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output second version of the model to csv\n",
    "results2.to_csv('results2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse is 1601.6889262847183\n",
      "rmspe is 0.21327051770936867\n"
     ]
    }
   ],
   "source": [
    "print('rmse is '+str(rmse(results2)))\n",
    "print('rmspe is '+str(rmspe(results2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "\n",
    "# fig1 = m.plot(forecast)\n",
    "# fig2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Don't Go Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try use Seq2Seq in order to see if this is a good way of forecasting. I am following this tutorial: https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_date = train.columns[1]\n",
    "data_end_date = train.columns[-1]\n",
    "print('Data ranges from %s to %s' % (data_start_date, data_end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_series(train, n_series):\n",
    "    \n",
    "    sample = train.sample(n_series, random_state=8)\n",
    "    page_labels = sample['Store'].tolist()\n",
    "    series_samples = sample.loc[:,data_start_date:data_end_date]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    for i in range(series_samples.shape[0]):\n",
    "        pd.Series(series_samples.iloc[i]).astype(np.float64).plot(linewidth=1.5)\n",
    "    \n",
    "    plt.title('Randomly Selected Daily Store Sales')\n",
    "    plt.legend(page_labels)\n",
    "    \n",
    "plot_random_series(train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "pred_steps = 14\n",
    "pred_length = timedelta(pred_steps)\n",
    "\n",
    "first_day = pd.to_datetime(data_start_date) \n",
    "last_day = pd.to_datetime(data_end_date)\n",
    "\n",
    "val_pred_start = last_day - pred_length + timedelta(1)\n",
    "val_pred_end = last_day\n",
    "\n",
    "train_pred_start = val_pred_start - pred_length\n",
    "train_pred_end = val_pred_start - timedelta(days=1)\n",
    "\n",
    "enc_length = train_pred_start - first_day\n",
    "\n",
    "train_enc_start = first_day\n",
    "train_enc_end = train_enc_start + enc_length - timedelta(1)\n",
    "\n",
    "val_enc_start = train_enc_start + pred_length\n",
    "val_enc_end = val_enc_start + enc_length - timedelta(1)\n",
    "\n",
    "print('Train encoding:', train_enc_start, '-', train_enc_end)\n",
    "print('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\n",
    "print('Val encoding:', val_enc_start, '-', val_enc_end)\n",
    "print('Val prediction:', val_pred_start, '-', val_pred_end)\n",
    "\n",
    "print('\\nEncoding interval:', enc_length.days)\n",
    "print('Prediction interval:', pred_length.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in train.columns[1:]]),\n",
    "                          data=[i for i in range(len(train.columns[1:]))])\n",
    "\n",
    "series_array = train[train.columns[1:]].values\n",
    "\n",
    "def get_time_block_series(series_array, date_to_index, start_date, end_date):\n",
    "    \n",
    "    inds = date_to_index[start_date:end_date]\n",
    "    return series_array[:,inds]\n",
    "\n",
    "def transform_series_encode(series_array):\n",
    "    \n",
    "    series_array = np.nan_to_num(series_array) # filling NaN with 0\n",
    "    series_mean = series_array.mean(axis=1).reshape(-1,1) \n",
    "    series_array = series_array - series_mean\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array, series_mean\n",
    "\n",
    "def transform_series_decode(series_array, encode_series_mean):\n",
    "    \n",
    "    series_array = series_array - encode_series_mean\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# convolutional layer parameters\n",
    "n_filters = 32\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(8)]\n",
    "\n",
    "# define an input history series and pass it through a stack of dilated causal convolutions. \n",
    "history_seq = Input(shape=(None, 1))\n",
    "x = history_seq\n",
    "\n",
    "for dilation_rate in dilation_rates:\n",
    "    x = Conv1D(filters=n_filters,\n",
    "               kernel_size=filter_width, \n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate)(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "# extract the last 14 time steps as the training target\n",
    "def slice(x, seq_length):\n",
    "    return x[:,-seq_length:,:]\n",
    "\n",
    "pred_seq_train = Lambda(slice, arguments={'seq_length':14})(x)\n",
    "\n",
    "model = Model(history_seq, pred_seq_train)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n_samples = 1000\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# sample of series from train_enc_start to train_enc_end  \n",
    "encoder_input_data = get_time_block_series(series_array, date_to_index, \n",
    "                                           train_enc_start, train_enc_end)[:first_n_samples]\n",
    "encoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n",
    "\n",
    "# sample of series from train_pred_start to train_pred_end \n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, \n",
    "                                            train_pred_start, train_pred_end)[:first_n_samples]\n",
    "decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n",
    "\n",
    "# okay, so for each one of these, \n",
    "# 1) for the encoder portion, we first take each series and subtract the mean\n",
    "# 2) for the decoder portion, we find the prediction (last 2 weeks) and then set that up with the same transformations\n",
    "\n",
    "# we append a lagged history of the target series to the input data, \n",
    "# so that we can train with teacher forcing\n",
    "lagged_target_history = decoder_target_data[:,:-1,:1]\n",
    "encoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n",
    "\n",
    "# here, we are adding the lagged history of the target series to the input data at the end\n",
    "# looks like, train then test-1 day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(), loss='mean_absolute_percentage_error')\n",
    "history = model.fit(encoder_input_data, \n",
    "                    decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Percentage Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend(['Train','Valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(input_sequence):\n",
    "\n",
    "    history_sequence = input_sequence.copy()\n",
    "    pred_sequence = np.zeros((1,pred_steps,1)) # initialize output (pred_steps time steps)  \n",
    "    \n",
    "    for i in range(pred_steps):\n",
    "        \n",
    "        # record next time step prediction (last time step of model output) \n",
    "        last_step_pred = model.predict(history_sequence)[0,-1,0]\n",
    "        pred_sequence[0,i,0] = last_step_pred\n",
    "        \n",
    "        # add the next time step prediction to the history sequence\n",
    "        history_sequence = np.concatenate([history_sequence, \n",
    "                                           last_step_pred.reshape(-1,1,1)], axis=1)\n",
    "\n",
    "    return pred_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\n",
    "encoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n",
    "\n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\n",
    "decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_series_mean[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(encoder_input_data, encode_series_mean, decoder_target_data, sample_ind, enc_tail_len=14):\n",
    "\n",
    "    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n",
    "    encode_series_mean = encode_series_mean[sample_ind:sample_ind+1] \n",
    "    pred_series = predict_sequence(encode_series)\n",
    "    \n",
    "    encode_series = encode_series.reshape(-1,1)\n",
    "    encode_series += encode_series_mean\n",
    "    pred_series = pred_series.reshape(-1,1)\n",
    "    pred_series += encode_series_mean\n",
    "    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n",
    "    target_series += encode_series_mean\n",
    "    \n",
    "    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n",
    "    x_encode = encode_series_tail.shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))   \n",
    "    \n",
    "    plt.plot(range(1,x_encode+1),encode_series_tail)\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n",
    "    \n",
    "    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n",
    "    plt.legend(['Encoding Series','Target Series','Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_plot(encoder_input_data, encode_series_mean, decoder_target_data,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issues with install: https://forums.fast.ai/t/fastai-v0-7-install-issues-thread/24652 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try version with actual LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jfpuget/Kaggle/blob/master/WebTrafficPrediction/keras_simple.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
