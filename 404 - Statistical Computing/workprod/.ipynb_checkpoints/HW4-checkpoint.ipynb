{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt \n",
    "* HW assignment: Using dataset selected for final project:\n",
    "\n",
    "\n",
    "* Perform feature engineering\n",
    "* Estimate baseline model\n",
    "* Estimate different model and/or different loss function to improve model performance\n",
    "* Hint: Determine what metric(s) is/are appropriate for your use case\n",
    "* Interpret results\n",
    "* Explain what you did and why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write-up\n",
    "* Goal: Forecast the last 6 weeks of the time series for each store.\n",
    "* Kaggle Prompt: Rossmann operates over 3,000 drug stores in 7 European countries. Currently, Rossmann store managers are tasked with predicting their daily sales for up to six weeks in advance. Store sales are influenced by many factors, including promotions, competition, school and state holidays, seasonality, and locality. With thousands of individual managers predicting sales based on their unique circumstances, the accuracy of results can be quite varied. In their first Kaggle competition, Rossmann is challenging you to predict 6 weeks of daily sales for 1,115 stores located across Germany. Reliable sales forecasts enable store managers to create effective staff schedules that increase productivity and motivation. By helping Rossmann create a robust prediction model, you will help store managers stay focused on what’s most important to them: their customers and their teams! \n",
    "\n",
    "**Variable Importance**\n",
    "\n",
    "I first utilize a GBM in order to see if it can quickly tell me which variables are important or not. I do not set this up as a normal time series forecasting question, so I don't add any lags (prev day, week, month). I also don't split the dataset by time series and I don't force the train/test to not overlap. However, using the output, I am able to see that the following variables are important: \n",
    "\n",
    "    1. \"Open\": if store is open\n",
    "    2. \"Promo\": if that Day is a promo day\n",
    "    3. \"CompetitionDistance\": nearest competitor distance\n",
    "    \n",
    "**Feature Engineering**\n",
    "\n",
    "I honestly couldn't think of a ton of different features to add. I did think that adding in that day's distance to the previous open day and the previous promo day could be promising, given how important those features seemed to be.\n",
    "\n",
    "**1st Attempt: Prophet w/o addtl regressors**\n",
    "As a baseline model, I decided to use <a href=\"https://research.fb.com/prophet-forecasting-at-scale/\">Prophet from fb</a>. It's a time series model that I use as a baseline model because it doesn't make a ton of assumptions about the underlying data structure.\n",
    "\n",
    "**2nd Attempt: Prophet w/ addtl regressors**\n",
    "Additionally, it is very simple to implement and has the ability to add in additional regressors. The second version of the model included the regressors for the top 3 important variables as well as some of the features that I created, \"daydiff_open_dt\" and \"daydiff_promo_dt\". These variables represent the distance from the date being forecast to the previous day the store was open as well as the previous day that the store had a promo.\n",
    "\n",
    "**Metric Selection**\n",
    "The competition utilizes RMSPE (Root Mean Squared Percentage Error) and doesn't include any day with 0 sales. I think that this is good in that it penalizes large errors. However, I do not like the fact that we are looking at the error relative to the prediction, so I also recommend looking at RMSE (without 0 sale days). Since the competition says the purpose is to help the managers effectively staff their stores, I make the assumption that there are a certain number of salespeople per sales volume. The problem with RMSPE is: if there were only 1 store/day and we forecast 50,000 in sales and the actual came in at 100,000, the RMSPE would be 50%. Now, if there were only 1 store/day and we forecast 50 in sales and the actual came in at 100, the RMSPE would **also** be 50%. To me, the first seems like a much bigger error in terms of staffing (if there is some $/salespeople ratio). RMSE for the first scenario is 50,000 and 50 for the second.\n",
    "\n",
    "**Interpreting Results**\n",
    "Manually looking at some of the forecast graphs, the model looks like it does a pretty good job of picking up on Day of Week Seasonality. Because of the granularity of the data, I will need to dig further into how to interpret results at the store-level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/andrewsang/Documents/ucla_stats/final_proj/workprod'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cd /home/paperspace/ANDREW-MINKYU-SANG/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/andrewsang/Documents/ucla_stats\n"
     ]
    }
   ],
   "source": [
    "cd /Users/andrewsang/Documents/ucla_stats/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mClass0-Intro\u001b[m\u001b[m/     \u001b[34mClass3\u001b[m\u001b[m/           README.md         requirements.txt\r\n",
      "\u001b[34mClass1\u001b[m\u001b[m/           \u001b[34mClass4\u001b[m\u001b[m/           \u001b[34mfinal_proj\u001b[m\u001b[m/\r\n",
      "\u001b[34mClass2\u001b[m\u001b[m/           \u001b[34mClass5\u001b[m\u001b[m/           req.txt\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/andrewsang/anaconda3/envs/stat404-final/lib/python3.6/site-packages/IPython/core/magic.py:187: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  call = lambda f, *a, **k: f(*a, **k)\n",
      "/Users/andrewsang/anaconda3/envs/stat404-final/lib/python3.6/site-packages/IPython/core/magics/execution.py:1135: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  all_runs = timer.repeat(repeat, number)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "997 ms ± 104 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit \n",
    "\n",
    "train = pd.read_csv('final_proj/input_data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "805 ms ± 61.4 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit\n",
    "\n",
    "typ_dict = {'Store':np.int8,\n",
    " 'DayOfWeek':np.int8,\n",
    " 'Date':object,\n",
    " 'Sales':np.int8,\n",
    " 'Customers':np.int8,\n",
    " 'Open':np.int8,\n",
    " 'Promo':np.int8,\n",
    " 'StateHoliday':object,\n",
    " 'SchoolHoliday':np.int8}\n",
    "\n",
    "train = pd.read_csv('final_proj/input_data/train.csv',dtype=typ_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding in the dictionary sped up the data load process. I did this because it seemed like something that would speed up the process. In a section below, I utilized multiprocessing in order to speed up a for-loop that I had been running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('final_proj/input_data/train.csv')\n",
    "test = pd.read_csv('final_proj/input_data/test.csv')\n",
    "store = pd.read_csv('final_proj/input_data/store.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.loc[:,'Date'] = pd.to_datetime(train['Date'])\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions for Data Manipulation and Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_manipulation(dataframe):\n",
    "    dataframe = dataframe.merge(store,how='left',on='Store')\n",
    "    mask = pd.isnull(dataframe['CompetitionDistance'])\n",
    "    # replace 3 stores with median where competition distance is null\n",
    "    dataframe.loc[mask,'CompetitionDistance'] = store['CompetitionDistance'].median() \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def day_dist(dataframe, start_dt):\n",
    "    # measuring current date - \n",
    "    dataframe = dataframe.sort_values(['Store','Date'])\n",
    "    dataframe.loc[:,'prev_'+start_dt] = dataframe.groupby('Store')[start_dt] \\\n",
    "                                                 .transform(lambda x:x.ffill().shift(1))\n",
    "    dataframe.loc[:,'daydiff_'+start_dt] = (dataframe['Date']- dataframe['prev_'+start_dt]) / \\\n",
    "                                            np.timedelta64(1, 'D') - 1\n",
    "    dataframe.loc[pd.isnull(dataframe['daydiff_'+start_dt]),'daydiff_'+start_dt] = np.NaN\n",
    "    return dataframe\n",
    "\n",
    "def ftr_eng(dataframe):\n",
    "    dataframe.loc[:,'promo_dt'] = np.where(dataframe['Promo']==1,dataframe['Date'], np.NaN)\n",
    "    dataframe.loc[:,'closed_dt'] = np.where(dataframe['Open']==0,dataframe['Date'], np.NaN)\n",
    "    dataframe.loc[:,'open_dt'] = np.where(dataframe['Open']==1,dataframe['Date'], np.NaN)\n",
    "    dataframe.head()\n",
    "    for k in ['Date','closed_dt','open_dt','promo_dt']:\n",
    "        dataframe.loc[:,k] = pd.to_datetime(dataframe.loc[:,k])\n",
    "        \n",
    "    dataframe = day_dist(dataframe,'open_dt') # distance to most recent open day\n",
    "    dataframe = day_dist(dataframe,'promo_dt') # distance to most recent promo day\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def piv_df(dataframe):\n",
    "    return pd.pivot_table(dataframe,values='Sales',index=['Store'], columns=['Date']).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use GBM for Variable Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('final_proj/input_data/train.csv')\n",
    "df = data_manipulation(df)\n",
    "df = ftr_eng(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import h2o\n",
    "from h2o.estimators.gbm import H2OGradientBoostingEstimator\n",
    "\n",
    "h2o.init()\n",
    "\n",
    "hf = h2o.H2OFrame(df)\n",
    "\n",
    "# Set up X and Y columns\n",
    "X = [e for e in df]\n",
    "X.remove('Store')\n",
    "X.remove('Sales')\n",
    "X.remove('Customers')\n",
    "X.remove('open_dt')\n",
    "X.remove('closed_dt')\n",
    "X.remove('promo_dt')\n",
    "y = 'Sales'\n",
    "\n",
    "# Split Frame\n",
    "train, valid, test = hf.split_frame([0.6, 0.2], seed=1234)\n",
    "\n",
    "# Specify Model\n",
    "gbm = H2OGradientBoostingEstimator(seed=123)\n",
    "gbm.train(X, y, training_frame=train, validation_frame=valid)\n",
    "\n",
    "# Summary\n",
    "gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prophetFormat(dataframe):\n",
    "    dataframe = dataframe.rename(index=str, columns={\"Sales\": \"y\",\"Date\": \"ds\"})\n",
    "    dataframe.loc[:,'ds'] = pd.to_datetime(dataframe['ds'])\n",
    "    for c in ['Promo','Open','CompetitionDistance']:\n",
    "        dataframe.loc[:,c] = dataframe.loc[:,c].astype(float)\n",
    "    dataframe.loc[:,'floor'] = 0\n",
    "    dataframe = dataframe.fillna(0)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fbprophet import Prophet\n",
    "\n",
    "og = pd.read_csv('final_proj/input_data/train.csv')\n",
    "og = data_manipulation(og)\n",
    "og = ftr_eng(og)\n",
    "og = prophetFormat(og)\n",
    "\n",
    "date_change = pd.to_datetime('2015-06-20') # this date leaves 42 days post\n",
    "mask = og['ds']>=date_change\n",
    "test = og[mask]\n",
    "train = og[mask==False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "store_list = train['Store'].unique().tolist()\n",
    "\n",
    "def run_indiv_forecast(idx, addtl_reg=False,return_model=False):\n",
    "    # pull data\n",
    "    sample = train.loc[train['Store']==store_list[idx],:]\n",
    "    test_sample = test.loc[test['Store']==store_list[idx],:]\n",
    "    \n",
    "    # create model, add regressors\n",
    "    m = Prophet(daily_seasonality=False)\n",
    "    if addtl_reg:\n",
    "        m.add_regressor('Promo',mode='multiplicative')\n",
    "        m.add_regressor('Open',mode='multiplicative')\n",
    "        m.add_regressor('CompetitionDistance')\n",
    "        m.add_regressor('daydiff_open_dt')\n",
    "        m.add_regressor('daydiff_promo_dt')\n",
    "    m.fit(sample)\n",
    "    \n",
    "    # create results, merge w actuals\n",
    "    forecast = m.predict(test_sample)\n",
    "    forecast.loc[:,'Store'] = train.loc[train['Store']==store_list[idx],'Store'].max()\n",
    "    fc = forecast[['Store','ds','yhat']]\n",
    "    fc = fc.merge(test_sample.loc[:,['ds','y']],how='left',on=['ds'])\n",
    "    if return_model:\n",
    "        return forecast, m\n",
    "    return fc\n",
    "\n",
    "def run_individ_forecast_w_reg(idx):\n",
    "    return run_indiv_forecast(idx,addtl_reg=True)\n",
    "\n",
    "def run_individ_forecast_w_reg_mod(idx):\n",
    "    return run_indiv_forecast(idx,addtl_reg=True,return_model=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st Attempt: Prophet w/o addtl regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parallelization comes in spirit from: https://medium.com/devschile/forecasting-multiples-time-series-using-prophet-in-parallel-2515abd1a245"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes a long time to run\n",
    "\n",
    "# import time\n",
    "\n",
    "# # Run Version without External Regressors\n",
    "# store_list = train.Store.unique()\n",
    "# start_time = time.time()\n",
    "\n",
    "# p = Pool(cpu_count())\n",
    "# seriesidx = np.arange(0,train['Store'].nunique())\n",
    "# predictions = list(p.imap(run_indiv_forecast, seriesidx))\n",
    "# p.close()\n",
    "# p.join()\n",
    "\n",
    "# print(\"-- %s seconds --\" % (time.time() - start_time))\n",
    "\n",
    "# results = pd.DataFrame()\n",
    "\n",
    "# for e in np.arange(0,len(predictions)):\n",
    "#     results = pd.concat([results,predictions[e]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output results to csv\n",
    "# results.to_csv('final_proj/workprod/results.csv')\n",
    "\n",
    "# Load in results via csv\n",
    "results = pd.read_csv('final_proj/workprod/results.csv')\n",
    "results = results.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Evaluation Metrics\n",
    "def rmse(dataframe):\n",
    "    interim = dataframe.loc[dataframe['y']!=0,:].groupby(['Store']).sum().reset_index()\n",
    "    return np.sqrt(np.mean((interim['y'] - interim['yhat'])**2))\n",
    "\n",
    "def rmspe(dataframe):\n",
    "    interim = dataframe.loc[dataframe['y']!=0,:].groupby(['Store']).sum().reset_index()\n",
    "    return np.sqrt(np.mean(((interim['y']-interim['yhat'])/interim['y'])**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print('rmse is '+str(rmse(results)))\n",
    "print('rmspe is '+str(rmspe(results)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd Attempt: Prophet w/ addtl regressors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Takes a while to run\n",
    "\n",
    "# # Run Version with External Regressors\n",
    "# store_list = train.Store.unique()\n",
    "# results2 = pd.DataFrame()\n",
    "\n",
    "# start_time = time.time()\n",
    "\n",
    "# p = Pool(cpu_count())\n",
    "# seriesidx = np.arange(0,train['Store'].nunique())\n",
    "# predictions = list(p.imap(run_individ_forecast_w_reg, seriesidx))\n",
    "# p.close()\n",
    "# p.join()\n",
    "\n",
    "# print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# results2 = pd.DataFrame()\n",
    "\n",
    "# for e in np.arange(0,len(predictions)):\n",
    "#     results2 = pd.concat([results2,predictions[e]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output second version of the model to csv\n",
    "# results.to_csv('final_proj/workprod/results2.csv')\n",
    "\n",
    "# Load in results via csv\n",
    "results2 = pd.read_csv('final_proj/workprod/results2.csv')\n",
    "results2 = results2.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff = results.groupby('Store').sum().reset_index()\n",
    "diff.loc[:,'err'] = diff['y'] - diff['yhat']\n",
    "diff.sort_values('err',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diff2 = results2.groupby('Store').sum().reset_index()\n",
    "diff2.loc[:,'err'] = diff2['y'] - diff2['yhat']\n",
    "diff2.sort_values('err',ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('rmse is '+str(rmse(results2)))\n",
    "print('rmspe is '+str(rmspe(results2)))\n",
    "\n",
    "# Slight improvement in rmse & rmspe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking at Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components\n",
    "\n",
    "forecast, m = run_individ_forecast_w_reg_mod(100)\n",
    "fig1 = m.plot(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2 = m.plot_components(forecast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo = results.merge(results2,how='left',on=['Store','ds'],suffixes=('_1', '_2'))\n",
    "combo.loc[:,'ds'] = pd.to_datetime(combo['ds'])\n",
    "combo = combo.merge(test,how='left',on=['Store','ds'],suffixes=('_pred','_test'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.loc[combo['y']!=0,['err_1','err_2']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.loc[combo['y']!=0,:].groupby('DayOfWeek')['err_1','err_2'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.loc[combo['y']!=0,:].groupby('Promo')['err_1','err_2'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.loc[combo['err_2']<-20000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combo.loc[:,'err_1'] = combo['y_1'] - combo['yhat_1']\n",
    "combo.loc[:,'err_2'] = combo['y_2'] - combo['yhat_2']\n",
    "combo.head(1).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please Don't Go Further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Model (LSTM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's try use Seq2Seq in order to see if this is a good way of forecasting. I am following this tutorial: https://github.com/JEddy92/TimeSeries_Seq2Seq/blob/master/notebooks/TS_Seq2Seq_Intro.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_start_date = train.columns[1]\n",
    "data_end_date = train.columns[-1]\n",
    "print('Data ranges from %s to %s' % (data_start_date, data_end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_series(train, n_series):\n",
    "    \n",
    "    sample = train.sample(n_series, random_state=8)\n",
    "    page_labels = sample['Store'].tolist()\n",
    "    series_samples = sample.loc[:,data_start_date:data_end_date]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))\n",
    "    \n",
    "    for i in range(series_samples.shape[0]):\n",
    "        pd.Series(series_samples.iloc[i]).astype(np.float64).plot(linewidth=1.5)\n",
    "    \n",
    "    plt.title('Randomly Selected Daily Store Sales')\n",
    "    plt.legend(page_labels)\n",
    "    \n",
    "plot_random_series(train, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "pred_steps = 14\n",
    "pred_length = timedelta(pred_steps)\n",
    "\n",
    "first_day = pd.to_datetime(data_start_date) \n",
    "last_day = pd.to_datetime(data_end_date)\n",
    "\n",
    "val_pred_start = last_day - pred_length + timedelta(1)\n",
    "val_pred_end = last_day\n",
    "\n",
    "train_pred_start = val_pred_start - pred_length\n",
    "train_pred_end = val_pred_start - timedelta(days=1)\n",
    "\n",
    "enc_length = train_pred_start - first_day\n",
    "\n",
    "train_enc_start = first_day\n",
    "train_enc_end = train_enc_start + enc_length - timedelta(1)\n",
    "\n",
    "val_enc_start = train_enc_start + pred_length\n",
    "val_enc_end = val_enc_start + enc_length - timedelta(1)\n",
    "\n",
    "print('Train encoding:', train_enc_start, '-', train_enc_end)\n",
    "print('Train prediction:', train_pred_start, '-', train_pred_end, '\\n')\n",
    "print('Val encoding:', val_enc_start, '-', val_enc_end)\n",
    "print('Val prediction:', val_pred_start, '-', val_pred_end)\n",
    "\n",
    "print('\\nEncoding interval:', enc_length.days)\n",
    "print('Prediction interval:', pred_length.days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_to_index = pd.Series(index=pd.Index([pd.to_datetime(c) for c in train.columns[1:]]),\n",
    "                          data=[i for i in range(len(train.columns[1:]))])\n",
    "\n",
    "series_array = train[train.columns[1:]].values\n",
    "\n",
    "def get_time_block_series(series_array, date_to_index, start_date, end_date):\n",
    "    \n",
    "    inds = date_to_index[start_date:end_date]\n",
    "    return series_array[:,inds]\n",
    "\n",
    "def transform_series_encode(series_array):\n",
    "    \n",
    "    series_array = np.nan_to_num(series_array) # filling NaN with 0\n",
    "    series_mean = series_array.mean(axis=1).reshape(-1,1) \n",
    "    series_array = series_array - series_mean\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array, series_mean\n",
    "\n",
    "def transform_series_decode(series_array, encode_series_mean):\n",
    "    \n",
    "    series_array = series_array - encode_series_mean\n",
    "    series_array = series_array.reshape((series_array.shape[0],series_array.shape[1], 1))\n",
    "    \n",
    "    return series_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Conv1D, Dense, Dropout, Lambda, concatenate\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "# convolutional layer parameters\n",
    "n_filters = 32\n",
    "filter_width = 2\n",
    "dilation_rates = [2**i for i in range(8)]\n",
    "\n",
    "# define an input history series and pass it through a stack of dilated causal convolutions. \n",
    "history_seq = Input(shape=(None, 1))\n",
    "x = history_seq\n",
    "\n",
    "for dilation_rate in dilation_rates:\n",
    "    x = Conv1D(filters=n_filters,\n",
    "               kernel_size=filter_width, \n",
    "               padding='causal',\n",
    "               dilation_rate=dilation_rate)(x)\n",
    "\n",
    "x = Dense(128, activation='relu')(x)\n",
    "x = Dropout(.5)(x)\n",
    "x = Dense(1)(x)\n",
    "\n",
    "# extract the last 14 time steps as the training target\n",
    "def slice(x, seq_length):\n",
    "    return x[:,-seq_length:,:]\n",
    "\n",
    "pred_seq_train = Lambda(slice, arguments={'seq_length':14})(x)\n",
    "\n",
    "model = Model(history_seq, pred_seq_train)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_n_samples = 1000\n",
    "batch_size = 128\n",
    "epochs = 10\n",
    "\n",
    "# sample of series from train_enc_start to train_enc_end  \n",
    "encoder_input_data = get_time_block_series(series_array, date_to_index, \n",
    "                                           train_enc_start, train_enc_end)[:first_n_samples]\n",
    "encoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n",
    "\n",
    "# sample of series from train_pred_start to train_pred_end \n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, \n",
    "                                            train_pred_start, train_pred_end)[:first_n_samples]\n",
    "decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)\n",
    "\n",
    "# okay, so for each one of these, \n",
    "# 1) for the encoder portion, we first take each series and subtract the mean\n",
    "# 2) for the decoder portion, we find the prediction (last 2 weeks) and then set that up with the same transformations\n",
    "\n",
    "# we append a lagged history of the target series to the input data, \n",
    "# so that we can train with teacher forcing\n",
    "lagged_target_history = decoder_target_data[:,:-1,:1]\n",
    "encoder_input_data = np.concatenate([encoder_input_data, lagged_target_history], axis=1)\n",
    "\n",
    "# here, we are adding the lagged history of the target series to the input data at the end\n",
    "# looks like, train then test-1 day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(), loss='mean_absolute_percentage_error')\n",
    "history = model.fit(encoder_input_data, \n",
    "                    decoder_target_data,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Mean Absolute Percentage Error Loss')\n",
    "plt.title('Loss Over Time')\n",
    "plt.legend(['Train','Valid'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sequence(input_sequence):\n",
    "\n",
    "    history_sequence = input_sequence.copy()\n",
    "    pred_sequence = np.zeros((1,pred_steps,1)) # initialize output (pred_steps time steps)  \n",
    "    \n",
    "    for i in range(pred_steps):\n",
    "        \n",
    "        # record next time step prediction (last time step of model output) \n",
    "        last_step_pred = model.predict(history_sequence)[0,-1,0]\n",
    "        pred_sequence[0,i,0] = last_step_pred\n",
    "        \n",
    "        # add the next time step prediction to the history sequence\n",
    "        history_sequence = np.concatenate([history_sequence, \n",
    "                                           last_step_pred.reshape(-1,1,1)], axis=1)\n",
    "\n",
    "    return pred_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = get_time_block_series(series_array, date_to_index, val_enc_start, val_enc_end)\n",
    "encoder_input_data, encode_series_mean = transform_series_encode(encoder_input_data)\n",
    "\n",
    "decoder_target_data = get_time_block_series(series_array, date_to_index, val_pred_start, val_pred_end)\n",
    "decoder_target_data = transform_series_decode(decoder_target_data, encode_series_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encode_series_mean[0:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_plot(encoder_input_data, encode_series_mean, decoder_target_data, sample_ind, enc_tail_len=14):\n",
    "\n",
    "    encode_series = encoder_input_data[sample_ind:sample_ind+1,:,:] \n",
    "    encode_series_mean = encode_series_mean[sample_ind:sample_ind+1] \n",
    "    pred_series = predict_sequence(encode_series)\n",
    "    \n",
    "    encode_series = encode_series.reshape(-1,1)\n",
    "    encode_series += encode_series_mean\n",
    "    pred_series = pred_series.reshape(-1,1)\n",
    "    pred_series += encode_series_mean\n",
    "    target_series = decoder_target_data[sample_ind,:,:1].reshape(-1,1) \n",
    "    target_series += encode_series_mean\n",
    "    \n",
    "    encode_series_tail = np.concatenate([encode_series[-enc_tail_len:],target_series[:1]])\n",
    "    x_encode = encode_series_tail.shape[0]\n",
    "    \n",
    "    plt.figure(figsize=(10,6))   \n",
    "    \n",
    "    plt.plot(range(1,x_encode+1),encode_series_tail)\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),target_series,color='orange')\n",
    "    plt.plot(range(x_encode,x_encode+pred_steps),pred_series,color='teal',linestyle='--')\n",
    "    \n",
    "    plt.title('Encoder Series Tail of Length %d, Target Series, and Predictions' % enc_tail_len)\n",
    "    plt.legend(['Encoding Series','Target Series','Predictions'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_plot(encoder_input_data, encode_series_mean, decoder_target_data,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "issues with install: https://forums.fast.ai/t/fastai-v0-7-install-issues-thread/24652 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try version with actual LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/jfpuget/Kaggle/blob/master/WebTrafficPrediction/keras_simple.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
