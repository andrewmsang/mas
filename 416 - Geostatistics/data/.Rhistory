L4lower_theory = -1.0 * L4upper
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
type="n",xlab="distance",ylab="L4(h) - h", main="L-function with Confidence bounds") +
points(s,L4,pch="*") +
lines(s,L4) +
lines(s,L4upper,lty=2,col="green",lwd=2) +
lines(s,L4lower,lty=2,col="green",lwd=2) +
lines(s,rep(0,length(s))) +
lines(s,L4upper_theory,lty=3,col="orange",lwd=2) +
lines(s,L4lower_theory,lty=3,col="orange",lwd=2)
### THEORETICAL BOUNDS for L-function
## bounds = 1.96 * sqrt(2*pi*A) * h / E(N), where
## A = area of space, and
## E(N) = expected # of pts in the space (approximated here using
## the observed # of pts
## J(r) = (1-G(r))/(1-F(r)).
## J = 1 corresponds to a stationary Poisson process.
## J < 1 indicates clustering. J > 1 indicates inhibition
b2 = as.ppp(b1, W = c(0,1,0,1))
j4 = Jest(b2)
plot(j4, main="J-function")
# ## or to control the plot yourself, try:
# plot(j4$r[j4$r<.3],j4$rs[j4$r<.3],xlab="h",ylab="J(h)",type="l",lty=1)
# lines(j4$r[j4$r<.3],j4$theo[j4$r<.3],lty=2)
# legend(.2,.2,lty=c(1,2),legend=c("data","Poisson"))
# Run Simulations
s = seq(.001,.3,length=50)
k4 = khat(b1,bdry,s)
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet=TRUE)
# K-function
# plot(c(0,max(s)),c(0,max(k4conf$upper,k4)), type="n",xlab="distance",ylab="K4(h)", main="K-function with Confidence bounds") +
#   points(s,k4,pch="*") +
#   lines(s,k4) +
#   lines(s,k4conf$upper,lty=3,col="green",lwd=2) +
#   lines(s,k4conf$lower,lty=3,col="green",lwd=2)
# L-function
L4 = sqrt(k4/pi)-s
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
L4upper_theory = 1.96 * sqrt(2*pi*1*1) * s / n
L4lower_theory = -1.0 * L4upper
par(mfrow=c(1,2))
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
type="n",xlab="distance",ylab="L4(h) - h", main="L-function with Confidence bounds") +
points(s,L4,pch="*") +
lines(s,L4) +
lines(s,L4upper,lty=2,col="green",lwd=2) +
lines(s,L4lower,lty=2,col="green",lwd=2) +
lines(s,rep(0,length(s))) +
lines(s,L4upper_theory,lty=3,col="orange",lwd=2) +
lines(s,L4lower_theory,lty=3,col="orange",lwd=2)
### THEORETICAL BOUNDS for L-function
## bounds = 1.96 * sqrt(2*pi*A) * h / E(N), where
## A = area of space, and
## E(N) = expected # of pts in the space (approximated here using
## the observed # of pts
## J(r) = (1-G(r))/(1-F(r)).
## J = 1 corresponds to a stationary Poisson process.
## J < 1 indicates clustering. J > 1 indicates inhibition
b2 = as.ppp(b1, W = c(0,1,0,1))
j4 = Jest(b2)
plot(j4, main="J-function")
# ## or to control the plot yourself, try:
# plot(j4$r[j4$r<.3],j4$rs[j4$r<.3],xlab="h",ylab="J(h)",type="l",lty=1)
# lines(j4$r[j4$r<.3],j4$theo[j4$r<.3],lty=2)
# legend(.2,.2,lty=c(1,2),legend=c("data","Poisson"))
# Run Simulations
# s = seq(.001,.3,length=50)
# k4 = khat(b1,bdry,s)
# k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet=TRUE)
# K-function
# plot(c(0,max(s)),c(0,max(k4conf$upper,k4)), type="n",xlab="distance",ylab="K4(h)", main="K-function with Confidence bounds") +
#   points(s,k4,pch="*") +
#   lines(s,k4) +
#   lines(s,k4conf$upper,lty=3,col="green",lwd=2) +
#   lines(s,k4conf$lower,lty=3,col="green",lwd=2)
# L-function
L4 = sqrt(k4/pi)-s
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
L4upper_theory = 1.96 * sqrt(2*pi*1*1) * s / n
L4lower_theory = -1.0 * L4upper
par(mfrow=c(1,2))
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
type="n",xlab="distance",ylab="L4(h) - h", main="L-function with Confidence bounds") +
points(s,L4,pch="*") +
lines(s,L4) +
lines(s,L4upper,lty=2,col="green",lwd=2) +
lines(s,L4lower,lty=2,col="green",lwd=2) +
lines(s,rep(0,length(s))) +
lines(s,L4upper_theory,lty=3,col="orange",lwd=2) +
lines(s,L4lower_theory,lty=3,col="orange",lwd=2)
### THEORETICAL BOUNDS for L-function
## bounds = 1.96 * sqrt(2*pi*A) * h / E(N), where
## A = area of space, and
## E(N) = expected # of pts in the space (approximated here using
## the observed # of pts
## J(r) = (1-G(r))/(1-F(r)).
## J = 1 corresponds to a stationary Poisson process.
## J < 1 indicates clustering. J > 1 indicates inhibition
b2 = as.ppp(b1, W = c(0,1,0,1))
j4 = Jest(b2)
plot(j4, main="J-function")
# ## or to control the plot yourself, try:
# plot(j4$r[j4$r<.3],j4$rs[j4$r<.3],xlab="h",ylab="J(h)",type="l",lty=1)
# lines(j4$r[j4$r<.3],j4$theo[j4$r<.3],lty=2)
# legend(.2,.2,lty=c(1,2),legend=c("data","Poisson"))
# https://www-ljk.imag.fr/membres/Jean-Francois.Coeurjolly/documents/lecture2.pdf
W = owin(c(0,1),c(0,1))
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
pp1 = as.ppp(df %>% mutate(x = norm_x, y = norm_y) %>% select(x, y), W)
fit1 = ppm(pp1, ~x + y)
fit1
# predict(fit1, locations=data.frame(x=seq(0.05,0.95,length=10), y=seq(0.05,0.95,length=10)))
# plot(simulate(fit1))
# generate_lambda_for_spaces(fit1, title="lambda for model 1")
logLik(fit1)
integral(residuals(fit1, type="pearson"))
plot(predict(fit1))
plot.msr(residuals(fit1, type="pearson"), equal.markscale = TRUE)
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
fit2 = ppm(pp1, ~polynom(x,y,2) + sqrt((x-0.5)**2+(y-.93)**2))
fit2 # view coefficients
plot(simulate(fit2))
generate_lambda_for_spaces(fit2)
# https://www-ljk.imag.fr/membres/Jean-Francois.Coeurjolly/documents/lecture2.pdf
W = owin(c(0,1),c(0,1))
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
pp1 = as.ppp(df %>% mutate(x = norm_x, y = norm_y) %>% select(x, y), W)
fit1 = ppm(pp1, ~x + y)
fit1
# predict(fit1, locations=data.frame(x=seq(0.05,0.95,length=10), y=seq(0.05,0.95,length=10)))
# plot(simulate(fit1))
# generate_lambda_for_spaces(fit1, title="lambda for model 1")
par(mfrow=c(1,2))
logLik(fit1)
integral(residuals(fit1, type="pearson"))
plot(predict(fit1))
plot.msr(residuals(fit1, type="pearson"), equal.markscale = TRUE)
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
fit2 = ppm(pp1, ~polynom(x,y,2) + sqrt((x-0.5)**2+(y-.93)**2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# https://www-ljk.imag.fr/membres/Jean-Francois.Coeurjolly/documents/lecture2.pdf
W = owin(c(0,1),c(0,1))
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
pp1 = as.ppp(df %>% mutate(x = norm_x, y = norm_y) %>% select(x, y), W)
fit1 = ppm(pp1, ~x + y)
fit1
# predict(fit1, locations=data.frame(x=seq(0.05,0.95,length=10), y=seq(0.05,0.95,length=10)))
# plot(simulate(fit1))
# generate_lambda_for_spaces(fit1, title="lambda for model 1")
# logLik(fit1)
# integral(residuals(fit1, type="pearson"))
par(mfrow=c(1,2))
plot(predict(fit1))
plot.msr(residuals(fit1, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
fit2 = ppm(pp1, ~polynom(x,y,2) + sqrt((x-0.5)**2+(y-.93)**2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
fit2 = ppm(pp1, ~polynom(x,y,2) + sqrt((x-0.5)**2+(y-.93)**2))
fit2 = ppm(pp1, ~polynom(x,y,2) )
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
plot.msr(residuals(fit1, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
# par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
# plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
# par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
# plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
# par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
# plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2)
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# par(mfrow=c(1,2))
plot(simulate(fit2))
# par(mfrow=c(1,2))
# plot(simulate(fit2))
generate_lambda_for_spaces(fit2)
# par(mfrow=c(1,2))
# plot(simulate(fit2))
generate_lambda_for_spaces(fit2)
# par(mfrow=c(1,2))
# plot(simulate(fit2))
generate_lambda_for_spaces(fit2)
# par(mfrow=c(1,2))
# plot(simulate(fit2))
generate_lambda_for_spaces(fit2, title="lambda for model 2")
# par(mfrow=c(1,2))
# plot(simulate(fit2))
generate_lambda_for_spaces(fit2, title="lambda for model 2")
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,3))
knitr::opts_chunk$set(echo = TRUE)
set.seed(1234)
## Libraries and Data Import
# set working directory
setwd("/Users/andrewsang/Dropbox/ucla/2019_09Fall/STATS416/Final Project/")
library(splancs)
library(dplyr)
library(spatstat)
library(spatial)
library(fields)
# import data
df = read.csv("data/shot_data_clean.csv")
print(df %>% dim())
# look at first few rows
df %>% head()
x1 = df$norm_x
y1 = df$norm_y
n  = length(x1)
plot(c(0,1),c(0,1), type="n", xlab="x-coordinate",ylab="y-coordinate", main="Lakers Shot Chart") +
points(x1,y1, col=rgb(red=0, green=0, blue=0, alpha=0.5))
par(mfrow=c(1,2))
bdw = sqrt(bw.nrd0(x1)^2+bw.nrd0(y1)^2)  # .05
b1 = as.points(x1,y1)
bdry = matrix(c(0,0,1,0,1,1,0,1,0,0),ncol=2,byrow=T)
z = kernel2d(b1,bdry,bdw)
image.plot(z,col=gray((64:20)/64),xlab="x",ylab="y", main="Lakers Shot Chart, Bandwidth:.05")
# + points(b1, col=rgb(red=0, green=0, blue=0, alpha=0.2))
bdw = .025 # override and choose own bandwidth
z = kernel2d(b1,bdry,bdw)
image.plot(z,col=gray((64:20)/64),xlab="x",ylab="y", main="Lakers Shot Chart, Bandwidth:.025")
# Run Simulations
s = seq(.001,.3,length=50)
k4 = khat(b1,bdry,s)
k4conf = Kenv.csr(npts(b1), bdry, 1000, s, quiet=TRUE)
# K-function
# plot(c(0,max(s)),c(0,max(k4conf$upper,k4)), type="n",xlab="distance",ylab="K4(h)", main="K-function with Confidence bounds") +
#   points(s,k4,pch="*") +
#   lines(s,k4) +
#   lines(s,k4conf$upper,lty=3,col="green",lwd=2) +
#   lines(s,k4conf$lower,lty=3,col="green",lwd=2)
# L-function
L4 = sqrt(k4/pi)-s
L4upper = sqrt(k4conf$upper/pi) - s
L4lower = sqrt(k4conf$lower/pi) - s
L4upper_theory = 1.96 * sqrt(2*pi*1*1) * s / n
L4lower_theory = -1.0 * L4upper
par(mfrow=c(1,2))
plot(c(0,max(s)),c(min(L4lower,L4),max(L4upper,L4)),
type="n",xlab="distance",ylab="L4(h) - h", main="L-function with Confidence bounds") +
points(s,L4,pch="*") +
lines(s,L4) +
lines(s,L4upper,lty=2,col="green",lwd=2) +
lines(s,L4lower,lty=2,col="green",lwd=2) +
lines(s,rep(0,length(s))) +
lines(s,L4upper_theory,lty=3,col="orange",lwd=2) +
lines(s,L4lower_theory,lty=3,col="orange",lwd=2)
### THEORETICAL BOUNDS for L-function
## bounds = 1.96 * sqrt(2*pi*A) * h / E(N), where
## A = area of space, and
## E(N) = expected # of pts in the space (approximated here using
## the observed # of pts
## J(r) = (1-G(r))/(1-F(r)).
## J = 1 corresponds to a stationary Poisson process.
## J < 1 indicates clustering. J > 1 indicates inhibition
b2 = as.ppp(b1, W = c(0,1,0,1))
j4 = Jest(b2)
plot(j4, main="J-function")
# ## or to control the plot yourself, try:
# plot(j4$r[j4$r<.3],j4$rs[j4$r<.3],xlab="h",ylab="J(h)",type="l",lty=1)
# lines(j4$r[j4$r<.3],j4$theo[j4$r<.3],lty=2)
# legend(.2,.2,lty=c(1,2),legend=c("data","Poisson"))
generate_lambda_for_spaces = function(fit_model, len=25, title="lambda", ...){
# Create matrix of placeholder spaces where we will compute lambda
x2 = seq(0.05,0.95,length=len)
y2 = seq(0.05,0.95,length=len)
zz2 = matrix(rep(0,(len*len)),ncol=len)
# loop through and predict for each value of
for(i in 1:len){
for(j in 1:len){
zz2[i,j] = predict(fit_model, locations=data.frame(x=x2[i], y=y2[j]))
}
}
# output plot
image.plot(x2,y2,zz2,xlab="x-coordinate", ylab="y-coordinate", main=title, col=gray((64:20)/64)) +
points(x1,y1)
# return matrix of lambdas
return(zz2)
}
# https://www-ljk.imag.fr/membres/Jean-Francois.Coeurjolly/documents/lecture2.pdf
W = owin(c(0,1),c(0,1))
# Here, the model that we are fitting is lambda(theta) of x,y = exp (theta_0 + theta_1 x + theta_2 y)
pp1 = as.ppp(df %>% mutate(x = norm_x, y = norm_y) %>% select(x, y), W)
fit1 = ppm(pp1, ~x + y)
fit1
# predict(fit1, locations=data.frame(x=seq(0.05,0.95,length=10), y=seq(0.05,0.95,length=10)))
# plot(simulate(fit1))
# generate_lambda_for_spaces(fit1, title="lambda for model 1")
# logLik(fit1)
# integral(residuals(fit1, type="pearson"))
par(mfrow=c(1,2))
plot(predict(fit1))
plot.msr(residuals(fit1, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,3))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
fit2 = ppm(pp1, ~polynom(x,y,4))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
fit2 = ppm(pp1, ~polynom(x,y,2))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
# plot(simulate(fit2))
generate_lambda_for_spaces(fit2, title="lambda for model 1")
fit2 = ppm(pp1, ~polynom(x,y,4))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,6))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,8))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
fit2 = ppm(pp1, ~polynom(x,y,20))
fit2 = ppm(pp1, ~polynom(x,y,12))
fit2 # view coefficients
fit2 = ppm(pp1, ~polynom(x,y,10))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
fit2 = ppm(pp1, ~polynom(x,y,8))
fit2 # view coefficients
par(mfrow=c(1,2))
# plot(simulate(fit2))
# generate_lambda_for_spaces(fit2, title="lambda for model 1")
plot(predict(fit2))
plot.msr(residuals(fit2, type="pearson"), equal.markscale = TRUE)
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
# residuals(fit1, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit1, type="pearson")
# residuals(fit2, type="pearson")$discrete %>% abs() %>% sum()
# diagnose.ppm(fit2, type="pearson")
results = do.call(rbind,
Map(data.frame,
pearson=c(integral(residuals(fit1,type="pearson")), integral(residuals(fit2,type="pearson"))),
logLik=c(logLik(fit1), logLik(fit2)),
deviance=c(deviance(fit1), deviance(fit2)),
AIC=c(AIC(fit1),AIC(fit2))))
row.names(results) = c('model1','model2')
results
